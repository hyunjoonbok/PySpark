{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Tabular Data Classification with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers a classification problem in Machine Learning and go through a comprehensive guide to succesfully develop an End-to-End ML class prediction model using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Algorithms**\n",
    "In order to predict the class of certain samples, there are several classification algorithms that can be used. In fact, when developing our machine learning models, we will train and evaluate a certain number of them, and we will keep those with better predicting performance. \\\n",
    "\n",
    "A non-exhaustive list of some of the most used algorithms are:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines\n",
    "- K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC**\n",
    "the metric that we will use in our project is the Reciever Operation Characteristic or ROC.\n",
    "The ROC curve tells us about how good the model can distinguish between two classes. It can get values from 0 to 1. The better the model is, the closer to 1 value it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a number of different supervised algorithms to precisely predict individuals’ income using data collected from the 1994 U.S. Census. \\\n",
    " \n",
    "We will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data.\n",
    "Our goal with this implementation is to build a model that accurately predicts whether an individual makes more than $50,000. \\\n",
    "\n",
    "As from our previous research we have found out that the individuals who are most likely to donate money to a charity are the ones that make more than $50,000. \\\n",
    "\n",
    "Therefore, we are facing a binary classification problem, where we want to determine wether an individual makes more than $50K a year (class 1) or do not (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init('C:/Users/bokhy/spark/spark-2.4.6-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "\n",
    "import pyspark # only run this after findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The census dataset consists of approximately 45222 data points, with each datapoint having 13 features.\n",
    "\n",
    "The dataset for this project can be found from the [UCI Machine Learning Repo](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Spark Session\n",
    "spark = SparkSession.builder.appName('imbalanced_binary_classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-L9PS4IG.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>imbalanced_binary_classification</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20d7f87a348>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, workClass: string, fnlwgt: int, education: string, education-num: int, marital-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: int, capital-loss: int, hours-per-week: int, native-country: string, income: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"./data/census.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"False\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# make sure to add column name as the CSV does not contain column name as default\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location) \\\n",
    "  .toDF(\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\"marital-status\", \"occupation\", \"relationship\",\n",
    "        \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "|age|       workClass|fnlwgt|   education|education-num|      marital-status|       occupation| relationship|              race|   sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "| 39|       State-gov| 77516|   Bachelors|           13|       Never-married|     Adm-clerical|Not-in-family|             White|  Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50|Self-emp-not-inc| 83311|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|         Private|215646|     HS-grad|            9|            Divorced|Handlers-cleaners|Not-in-family|             White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|         Private|234721|        11th|            7|  Married-civ-spouse|Handlers-cleaners|      Husband|             Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|         Private|338409|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|         Wife|             Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
      "| 37|         Private|284582|     Masters|           14|  Married-civ-spouse|  Exec-managerial|         Wife|             White|Female|           0|           0|            40| United-States| <=50K|\n",
      "| 49|         Private|160187|         9th|            5|Married-spouse-ab...|    Other-service|Not-in-family|             Black|Female|           0|           0|            16|       Jamaica| <=50K|\n",
      "| 52|Self-emp-not-inc|209642|     HS-grad|            9|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            45| United-States|  >50K|\n",
      "| 31|         Private| 45781|     Masters|           14|       Never-married|   Prof-specialty|Not-in-family|             White|Female|       14084|           0|            50| United-States|  >50K|\n",
      "| 42|         Private|159449|   Bachelors|           13|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|        5178|           0|            40| United-States|  >50K|\n",
      "| 37|         Private|280464|Some-college|           10|  Married-civ-spouse|  Exec-managerial|      Husband|             Black|  Male|           0|           0|            80| United-States|  >50K|\n",
      "| 30|       State-gov|141297|   Bachelors|           13|  Married-civ-spouse|   Prof-specialty|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|         India|  >50K|\n",
      "| 23|         Private|122272|   Bachelors|           13|       Never-married|     Adm-clerical|    Own-child|             White|Female|           0|           0|            30| United-States| <=50K|\n",
      "| 32|         Private|205019|  Assoc-acdm|           12|       Never-married|            Sales|Not-in-family|             Black|  Male|           0|           0|            50| United-States| <=50K|\n",
      "| 40|         Private|121772|   Assoc-voc|           11|  Married-civ-spouse|     Craft-repair|      Husband|Asian-Pac-Islander|  Male|           0|           0|            40|             ?|  >50K|\n",
      "| 34|         Private|245487|     7th-8th|            4|  Married-civ-spouse| Transport-moving|      Husband|Amer-Indian-Eskimo|  Male|           0|           0|            45|        Mexico| <=50K|\n",
      "| 25|Self-emp-not-inc|176756|     HS-grad|            9|       Never-married|  Farming-fishing|    Own-child|             White|  Male|           0|           0|            35| United-States| <=50K|\n",
      "| 32|         Private|186824|     HS-grad|            9|       Never-married|Machine-op-inspct|    Unmarried|             White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 38|         Private| 28887|        11th|            7|  Married-civ-spouse|            Sales|      Husband|             White|  Male|           0|           0|            50| United-States| <=50K|\n",
      "| 43|Self-emp-not-inc|292175|     Masters|           14|            Divorced|  Exec-managerial|    Unmarried|             White|Female|           0|           0|            45| United-States|  >50K|\n",
      "+---+----------------+------+------------+-------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workClass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'native-country',\n",
       " '>50K']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pyspark functions\n",
    "from pyspark.sql import functions as F\n",
    "# Create add new column to the dataset\n",
    "df = df.withColumn('>50K', F.when(df.income == '<=50K', 0).otherwise(1))\n",
    "# Drop the Income label\n",
    "df = df.drop('income')\n",
    "# Show dataset's columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing Numerical Features and One-Hot Encodin Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical features\n",
    "categorical_columns = [\n",
    " 'workClass',\n",
    " 'education',\n",
    " 'marital-status',\n",
    " 'occupation',\n",
    " 'relationship',\n",
    " 'race',\n",
    " 'sex',\n",
    " 'hours-per-week',\n",
    " 'native-country',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier, GBTClassifier, RandomForestClassifier, LogisticRegression)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns]\n",
    "# The encode of indexed values multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code basically indexes each categorical column using the StringIndexer, and then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the categorical encoded features with the numerical ones and make a vector with both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing encoded values\n",
    "categorical_encoded = [encoder.getOutputCol() for encoder in encoders]\n",
    "numerical_columns = ['age', 'education-num', 'capital-gain', 'capital-loss']\n",
    "inputcols = categorical_encoded + numerical_columns\n",
    "assembler = VectorAssembler(inputCols=inputcols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a pipeline to automatize this stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, workClass: string, fnlwgt: int, education: string, education-num: int, marital-status: string, occupation: string, relationship: string, race: string, sex: string, capital-gain: int, capital-loss: int, hours-per-week: int, native-country: string, >50K: int, workClass_indexed: double, education_indexed: double, marital-status_indexed: double, occupation_indexed: double, relationship_indexed: double, race_indexed: double, sex_indexed: double, hours-per-week_indexed: double, native-country_indexed: double, workClass_indexed_encoded: vector, education_indexed_encoded: vector, marital-status_indexed_encoded: vector, occupation_indexed_encoded: vector, relationship_indexed_encoded: vector, race_indexed_encoded: vector, sex_indexed_encoded: vector, hours-per-week_indexed_encoded: vector, native-country_indexed_encoded: vector, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model = pipeline.fit(df)\n",
    "# Transform data\n",
    "transformed = model.transform(df)\n",
    "display(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we will select a dataset only with the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "final_data = transformed.select('features', '>50K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classification models\n",
    "# Decision Trees\n",
    "# Random Forests\n",
    "# Gradient Boosted Trees\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='>50K', featuresCol='features')\n",
    "\n",
    "rfc = RandomForestClassifier(numTrees=150, labelCol='>50K', featuresCol='features')\n",
    "\n",
    "gbt = GBTClassifier(labelCol='>50K', featuresCol='features', maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39010\n",
      "9832\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# We will perform a classic 80/20 split between training and testing data.\n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2], seed=623)\n",
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaludate with Test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbt_preds = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating Model’s Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our evaluator will be the ROC\n",
    "my_eval = BinaryClassificationEvaluator(labelCol='>50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.5849312593442992\n"
     ]
    }
   ],
   "source": [
    "# Display Decision Tree evaluation metric\n",
    "print('DTC')\n",
    "print(my_eval.evaluate(dtc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC\n",
      "0.8914577709920453\n"
     ]
    }
   ],
   "source": [
    "# Display Random Forest evaluation metric\n",
    "print('RFC')\n",
    "print(my_eval.evaluate(rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT\n",
      "0.9044179860557597\n"
     ]
    }
   ],
   "source": [
    "# Display Gradien Boosting Tree evaluation metric\n",
    "print('GBT')\n",
    "print(my_eval.evaluate(gbt_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Improving Models Performance (Model Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to do this by performing the grid search cross validation technique. With it, we will evaluate the performance of the model with different combinations of previously sets of hyperparameter’s values.\n",
    "\n",
    "The hyperparameters that we will tune are:\n",
    "\n",
    "- Max Depth\n",
    "- Max Bins\n",
    "- Max Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9143539096589867"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Set the Parameters grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 60])\n",
    "             .addGrid(gbt.maxIter, [10, 20])\n",
    "             .build())\n",
    "\n",
    "# Iinitializing the cross validator class\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=my_eval, numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees\n",
    "cvModel = cv.fit(train_data)\n",
    "gbt_predictions_2 = cvModel.transform(test_data)\n",
    "my_eval.evaluate(gbt_predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also access the model's feature weights and intercepts easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Intercept: ', cvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cvModel.bestModel.coefficients\n",
    "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "display(weightsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best model's predictions and probabilities of each prediction class\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
